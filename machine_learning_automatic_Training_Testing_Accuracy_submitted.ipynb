{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.1 MB 1.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting matplotlib\n",
      "  Downloading matplotlib-3.7.5-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (9.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.2 MB 38.4 MB/s eta 0:00:01     |████████████████                | 4.6 MB 38.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "\u001b[K     |████████████████████████████████| 294 kB 50.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in ./.local/lib/python3.8/site-packages (1.24.4)\n",
      "Collecting scipy>=1.5.0\n",
      "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 34.5 MB 8.2 kB/s  eta 0:00:01    |██▍                             | 2.6 MB 38.8 MB/s eta 0:00:01�██████▋              | 19.0 MB 38.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Collecting joblib>=1.1.1\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "\u001b[K     |████████████████████████████████| 301 kB 36.6 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in ./.local/lib/python3.8/site-packages (from matplotlib) (24.1)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading pillow-10.4.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 36.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pyparsing>=2.3.1\n",
      "  Downloading pyparsing-3.1.4-py3-none-any.whl (104 kB)\n",
      "\u001b[K     |████████████████████████████████| 104 kB 53.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.7-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 36.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: importlib-resources>=3.2.0; python_version < \"3.10\" in ./.local/lib/python3.8/site-packages (from matplotlib) (6.4.5)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.1.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[K     |████████████████████████████████| 301 kB 36.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in ./.local/lib/python3.8/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.54.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.7 MB 16.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pandas>=1.2 in ./.local/lib/python3.8/site-packages (from seaborn) (2.0.3)\n",
      "Requirement already satisfied: zipp>=3.1.0; python_version < \"3.10\" in ./.local/lib/python3.8/site-packages (from importlib-resources>=3.2.0; python_version < \"3.10\"->matplotlib) (3.20.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.14.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.local/lib/python3.8/site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.local/lib/python3.8/site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Installing collected packages: scipy, threadpoolctl, joblib, scikit-learn, cycler, pillow, pyparsing, kiwisolver, contourpy, fonttools, matplotlib, seaborn\n",
      "Successfully installed contourpy-1.1.1 cycler-0.12.1 fonttools-4.54.1 joblib-1.4.2 kiwisolver-1.4.7 matplotlib-3.7.5 pillow-10.4.0 pyparsing-3.1.4 scikit-learn-1.3.2 scipy-1.10.1 seaborn-0.13.2 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn matplotlib seaborn numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genuine dataset\n",
    "genuine_data = pd.read_csv(\"genuine_with_label.csv\")\n",
    "\n",
    "# Malicious dataset\n",
    "malicious_data = pd.read_csv(\"malicious_with_label_spoofed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined data shape: (0, 4)\n",
      "NaN values in target variable (Attack): 0\n",
      "NaN values in feature variables:\n",
      " Duration                  0.0\n",
      "Received echo requests    0.0\n",
      "label                     0.0\n",
      "Attack                    0.0\n",
      "dtype: float64\n",
      "Shape after dropping NaN values: (0, 4)\n",
      "X shape: (0, 3)\n",
      "y shape: (0,)\n",
      "Error: X or y is empty. Check your data preparation.\n"
     ]
    }
   ],
   "source": [
    "# Check combined_data shape\n",
    "print(\"Combined data shape:\", combined_data.shape)\n",
    "\n",
    "# Check for NaN values in the target variable\n",
    "print(\"NaN values in target variable (Attack):\", combined_data['Attack'].isna().sum())\n",
    "\n",
    "# Check for NaN values in feature variables\n",
    "print(\"NaN values in feature variables:\\n\", combined_data.isna().sum())\n",
    "\n",
    "# Remove rows with NaN values\n",
    "combined_data = combined_data.dropna()\n",
    "\n",
    "# Check combined_data shape after removing NaN values\n",
    "print(\"Shape after dropping NaN values:\", combined_data.shape)\n",
    "\n",
    "# Prepare X and y\n",
    "X = combined_data.drop(['Attack'], axis=1)\n",
    "y = combined_data['Attack']\n",
    "\n",
    "# Check if X and y are empty\n",
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "\n",
    "# Proceed with splitting the dataset if X and y are non-empty\n",
    "if X.shape[0] > 0 and y.shape[0] > 0:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=177, stratify=y)\n",
    "\n",
    "    # Scaling the data\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Continue with model training...\n",
    "else:\n",
    "    print(\"Error: X or y is empty. Check your data preparation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input y contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m X \u001b[38;5;241m=\u001b[39m combined_data\u001b[38;5;241m.\u001b[39mdrop([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttack\u001b[39m\u001b[38;5;124m'\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     26\u001b[0m y \u001b[38;5;241m=\u001b[39m combined_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttack\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 28\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m177\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mstratify\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m X_train \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(X_train)\n\u001b[1;32m     30\u001b[0m X_test \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X_test)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/_param_validation.py:214\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    210\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    211\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    212\u001b[0m         )\n\u001b[1;32m    213\u001b[0m     ):\n\u001b[0;32m--> 214\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    224\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_split.py:2670\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2666\u001b[0m         CVClass \u001b[38;5;241m=\u001b[39m ShuffleSplit\n\u001b[1;32m   2668\u001b[0m     cv \u001b[38;5;241m=\u001b[39m CVClass(test_size\u001b[38;5;241m=\u001b[39mn_test, train_size\u001b[38;5;241m=\u001b[39mn_train, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[0;32m-> 2670\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstratify\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   2672\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   2673\u001b[0m     chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m   2674\u001b[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[1;32m   2675\u001b[0m     )\n\u001b[1;32m   2676\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_split.py:2229\u001b[0m, in \u001b[0;36mStratifiedShuffleSplit.split\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m   2195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, groups\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m   2196\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate indices to split data into training and test set.\u001b[39;00m\n\u001b[1;32m   2197\u001b[0m \n\u001b[1;32m   2198\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2227\u001b[0m \u001b[38;5;124;03m    to an integer.\u001b[39;00m\n\u001b[1;32m   2228\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2229\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43my\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39msplit(X, y, groups)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:957\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    952\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    953\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    954\u001b[0m         )\n\u001b[1;32m    956\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 957\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    965\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:122\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py:171\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    170\u001b[0m     )\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input y contains NaN."
     ]
    }
   ],
   "source": [
    "number_range = [50,100,150,200,250,300,350,400,450,500]\n",
    "\n",
    "LR_acuracy_AND_estimationTime = [[\"Train_Estimation\",\"Test_Estimation\",\"Accuracy\"]]\n",
    "LR_confusionMatrix = [['TN','FP','FN',\"TP\"]]\n",
    "\n",
    "KNN_acuracy_AND_estimationTime = [[\"Train_Estimation\",\"Test_Estimation\",\"Accuracy\"]]\n",
    "KNN_confusionMatrix = [['TN','FP','FN',\"TP\"]]\n",
    "\n",
    "SVM_acuracy_AND_estimationTime = [[\"Train_Estimation\",\"Test_Estimation\",\"Accuracy\"]]\n",
    "SVM_confusionMatrix = [['TN','FP','FN',\"TP\"]]\n",
    "\n",
    "\n",
    "\n",
    "for i in number_range:\n",
    "    combined_data= pd.concat([genuine_data.iloc[0:i],malicious_data.iloc[0:i]]).reset_index(drop=True)\n",
    "    # combined_data= pd.concat([genuine_data,malicious_data]).reset_index(drop=True)\n",
    "    # combined_data= pd.concat([genuine_data.iloc[0:150],malicious_data.iloc[0:150]]).reset_index(drop=True)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "#     combined_data[['Duration', 'Received echo requests']] = scaler.fit_transform(combined_data[['Duration', 'Received echo requests']])\n",
    "    combined_data.head()\n",
    "\n",
    "\n",
    "\n",
    "    X = combined_data.drop(['Attack'], axis=1)\n",
    "    y = combined_data['Attack']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=177,stratify =y)\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    ## Logistic Regression ########################\n",
    "\n",
    "\n",
    "    #Time for Training the model\n",
    "    logmodel = LogisticRegression()\n",
    "\n",
    "    start_counter_ns = time.perf_counter()\n",
    "\n",
    "    logmodel.fit(X_train,y_train)\n",
    "\n",
    "    end_counter_ns = time.perf_counter()\n",
    "    LR_Taining_timer_ns = end_counter_ns - start_counter_ns\n",
    "\n",
    "    #-----------------\n",
    "\n",
    "    #Time for testing/Predicting the data\n",
    "\n",
    "    start_counter_ns = time.perf_counter()\n",
    "\n",
    "    predictions = logmodel.predict(X_test)\n",
    "\n",
    "    end_counter_ns = time.perf_counter()\n",
    "    LR_Testing_timer_ns = end_counter_ns - start_counter_ns\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_test,predictions)\n",
    "    LR_acuracy_AND_estimationTime.append([LR_Taining_timer_ns,LR_Testing_timer_ns,accuracy])\n",
    "    CM = confusion_matrix(y_test,predictions)\n",
    "\n",
    "    CM_temp_array = []\n",
    "    for m in CM:\n",
    "        for j in m:\n",
    "            CM_temp_array.append(j)\n",
    "    LR_confusionMatrix.append(CM_temp_array)\n",
    "\n",
    "\n",
    "\n",
    "    ## K-NN########################\n",
    "\n",
    "    #Time for Training the model\n",
    "    knn  = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "    start_counter_ns = time.perf_counter()\n",
    "\n",
    "    knn.fit(X_train,y_train)\n",
    "\n",
    "    end_counter_ns = time.perf_counter()\n",
    "    KNN_Training_timer_ns = end_counter_ns - start_counter_ns\n",
    "\n",
    "    #----------------------------\n",
    "\n",
    "    #Time for testing/Predicting the data\n",
    "\n",
    "    start_counter_ns = time.perf_counter()\n",
    "\n",
    "    pred = knn.predict(X_test)\n",
    "\n",
    "\n",
    "    end_counter_ns = time.perf_counter()\n",
    "    KNN_Testing_timer_ns = end_counter_ns - start_counter_ns\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_test,pred)\n",
    "    KNN_acuracy_AND_estimationTime.append([KNN_Training_timer_ns,KNN_Testing_timer_ns,accuracy])\n",
    "\n",
    "    CM = confusion_matrix(y_test,pred)\n",
    "    CM_temp_array = []\n",
    "    for m in CM:\n",
    "        for j in m:\n",
    "            CM_temp_array.append(j)\n",
    "    KNN_confusionMatrix.append(CM_temp_array)\n",
    "\n",
    "\n",
    "    ## SVM ###################\n",
    "\n",
    "    #Time for Training the model\n",
    "    model = SVC()\n",
    "\n",
    "    start_counter_ns = time.perf_counter()\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    end_counter_ns = time.perf_counter()\n",
    "    SVM_train_timer_ns = end_counter_ns - start_counter_ns\n",
    "\n",
    "    #-------------------------------\n",
    "\n",
    "    #Time for testing/Predicting the data\n",
    "\n",
    "    start_counter_ns = time.perf_counter()\n",
    "\n",
    "    svmPrediction = model.predict(X_test)\n",
    "    confusion_matrix(y_test,svmPrediction)\n",
    "\n",
    "\n",
    "    end_counter_ns = time.perf_counter()\n",
    "    SVM_Test_timer_ns = end_counter_ns - start_counter_ns\n",
    "\n",
    "    accuracy = metrics.accuracy_score(y_test,svmPrediction)\n",
    "    SVM_acuracy_AND_estimationTime.append([SVM_train_timer_ns,SVM_Test_timer_ns,accuracy])\n",
    "\n",
    "\n",
    "    CM = confusion_matrix(y_test,svmPrediction)\n",
    "    CM_temp_array = []\n",
    "    for m in CM:\n",
    "        for j in m:\n",
    "            CM_temp_array.append(j)\n",
    "    SVM_confusionMatrix.append(CM_temp_array)\n",
    "    \n",
    "\n",
    "LR_CM = pd.DataFrame(LR_confusionMatrix)\n",
    "LR_CM.to_csv('1-LR_CM.csv',header = False, index= False)\n",
    "\n",
    "KNN_CM = pd.DataFrame(KNN_confusionMatrix)\n",
    "KNN_CM.to_csv('2-KNN_CM.csv',header = False, index= False)\n",
    "\n",
    "SVM_CM = pd.DataFrame(SVM_confusionMatrix)\n",
    "SVM_CM.to_csv('3-SVM_CM.csv',header = False, index= False)\n",
    "\n",
    "\n",
    "LR_A_EstimationTime = pd.DataFrame(LR_acuracy_AND_estimationTime)\n",
    "LR_A_EstimationTime.to_csv('1-LR_Accuracy_EstimationTime.csv',header = False, index= False)\n",
    "\n",
    "KNN_A_EstimationTime = pd.DataFrame(KNN_acuracy_AND_estimationTime)\n",
    "KNN_A_EstimationTime.to_csv('2-KNN_Accuracy_EstimationTime.csv',header = False, index= False)\n",
    "\n",
    "SVM_A_EstimationTime = pd.DataFrame(SVM_acuracy_AND_estimationTime)\n",
    "SVM_A_EstimationTime.to_csv('3-SVM_Accuracy_EstimationTime.csv',header = False, index= False)\n",
    "\n",
    "print('Array exported to file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross fold validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The cell below needs to be run to scale the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "combined_data[['Duration', 'Received echo requests']] = scaler.fit_transform(combined_data[['Duration', 'Received echo requests']])\n",
    "\n",
    "X = combined_data.drop(['Attack'], axis=1)\n",
    "y = combined_data['Attack']\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "def cross_validation(model, _X, _y, _cv=5):\n",
    "      '''Function to perform 5 Folds Cross-Validation\n",
    "       Parameters\n",
    "       ----------\n",
    "      model: Python Class, default=None\n",
    "              This is the machine learning algorithm to be used for training.\n",
    "      _X: array\n",
    "           This is the matrix of features.\n",
    "      _y: array\n",
    "           This is the target variable.\n",
    "      _cv: int, default=5\n",
    "          Determines the number of folds for cross-validation.\n",
    "       Returns\n",
    "       -------\n",
    "       The function returns a dictionary containing the metrics 'accuracy', 'precision',\n",
    "       'recall', 'f1' for both training set and validation set.\n",
    "      '''\n",
    "      _scoring = ['accuracy', 'precision', 'recall', 'f1']\n",
    "      results = cross_validate(estimator=model,\n",
    "                               X=_X,\n",
    "                               y=_y,\n",
    "                               cv=_cv,\n",
    "                               scoring=_scoring,\n",
    "                               return_train_score=True)\n",
    "      \n",
    "      return {\"Training Accuracy scores\": results['train_accuracy'],\n",
    "              \"Mean Training Accuracy\": results['train_accuracy'].mean()*100,\n",
    "              \"Training Precision scores\": results['train_precision'],\n",
    "              \"Mean Training Precision\": results['train_precision'].mean(),\n",
    "              \"Training Recall scores\": results['train_recall'],\n",
    "              \"Mean Training Recall\": results['train_recall'].mean(),\n",
    "              \"Training F1 scores\": results['train_f1'],\n",
    "              \"Mean Training F1 Score\": results['train_f1'].mean(),\n",
    "              \"Validation Accuracy scores\": results['test_accuracy'],\n",
    "              \"Mean Validation Accuracy\": results['test_accuracy'].mean()*100,\n",
    "              \"Validation Precision scores\": results['test_precision'],\n",
    "              \"Mean Validation Precision\": results['test_precision'].mean()*100,\n",
    "              \"Validation Recall scores\": results['test_recall'],\n",
    "              \"Mean Validation Recall\": results['test_recall'].mean()*100,\n",
    "              \"Validation F1 scores\": results['test_f1'],\n",
    "              \"Mean Validation F1 Score\": results['test_f1'].mean()*100\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LR_result = cross_validation(LogisticRegression(), X, y, 10)\n",
    "LR_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_result = cross_validation(KNeighborsClassifier(n_neighbors=5), X, y, 10)\n",
    "KNN_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_result = cross_validation(SVC(), X, y, 10)\n",
    "SVM_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
